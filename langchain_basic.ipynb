{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1217cc59",
   "metadata": {},
   "source": [
    "# LangChain Basics\n",
    "\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d47c762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Model name\n",
    "MODEL_NAME = \"gpt-5-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0577c0",
   "metadata": {},
   "source": [
    "## 2. Calling a Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d81372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Briefly — treat a language model as a powerful, flexible assistant, not an oracle. Key points to keep in mind:\n",
      "\n",
      "1. Purpose & expectations\n",
      "   - Models are excellent at generating text, summarizing, rewriting, brainstorming, and following instructions, but they do not \"know\" in a human sense and can be confidently wrong.\n",
      "\n",
      "2. Hallucinations and factual errors\n",
      "   - Models can invent facts, references, or quotes. Always verify critical information against trusted sources.\n",
      "\n",
      "3. Limitations of knowledge and currency\n",
      "   - A model’s training cut‑off or update schedule means it may lack recent facts or changing context. Check dates and version info.\n",
      "\n",
      "4. Bias and fairness\n",
      "   - Outputs can reflect training data biases. Anticipate and check for biased, offensive, or discriminatory content, especially in sensitive domains.\n",
      "\n",
      "5. Prompt design matters\n",
      "   - Clear, specific prompts with required format, constraints, and examples (few‑shot) produce better results. Use system/role framing when available.\n",
      "\n",
      "6. Temperature and decoding control\n",
      "   - Sampling parameters (temperature, top-p) affect creativity vs. determinism. Lower temperature → more predictable answers; higher → more varied.\n",
      "\n",
      "7. Ask for sources and chain of thought carefully\n",
      "   - Request citations, reasoning steps, or evidence, but treat chain-of-thought as a model’s internal reasoning that may still be flawed. Prefer verifiable citations.\n",
      "\n",
      "8. Privacy and data handling\n",
      "   - Don’t send sensitive personal, proprietary, or regulated data without understanding the service’s retention and privacy policies. Mask or anonymize when possible.\n",
      "\n",
      "9. Safety and misuse\n",
      "   - Prevent misuse (e.g., generating harmful instructions, disallowed content). Apply content filters and human review for risky outputs.\n",
      "\n",
      "10. Human-in-the-loop\n",
      "    - Use human review for high-stakes decisions (legal, medical, financial). Treat model outputs as drafts or suggestions to be validated.\n",
      "\n",
      "11. Testing and monitoring\n",
      "    - Evaluate outputs with domain tests, adversarial prompts, and user feedback. Monitor performance drift and edge cases after deployment.\n",
      "\n",
      "12. Cost, latency, and token limits\n",
      "    - Be aware of API costs, rate limits, and token size limits. Optimize prompts and use batching when appropriate.\n",
      "\n",
      "13. Versioning and reproducibility\n",
      "    - Log model version, prompt, parameters, and outputs for debugging, auditing, and reproducibility.\n",
      "\n",
      "14. Legal and intellectual property\n",
      "    - Consider copyright, licensing, and terms of service. When using generated text commercially, verify any legal constraints.\n",
      "\n",
      "15. Fail-safes and fallback strategies\n",
      "    - Provide graceful degradation (e.g., “I don’t know” handling), allow user corrections, and have human escalation paths.\n",
      "\n",
      "Quick checklist before using a model:\n",
      "- Is the task appropriate for an LLM?\n",
      "- Have you written a clear prompt and specified format?\n",
      "- Are you protecting sensitive data?\n",
      "- Will you verify outputs and cite sources?\n",
      "- Is there a human review for critical decisions?\n",
      "\n",
      "If you want, I can tailor these points to a specific use case (customer support, medical triage, coding, content creation), or give sample prompts and parameter settings.\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = ChatOpenAI(model=MODEL_NAME)\n",
    "\n",
    "# Set the question (prompt)\n",
    "user_prompt = \"What are the key points to keep in mind when using a language model?\"\n",
    "messages = [HumanMessage(content=user_prompt)]\n",
    "\n",
    "# Call the language model\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# Display the result\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18ca68",
   "metadata": {},
   "source": [
    "## 3. Setting System Prompts and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3896d741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the key points to remember when using a language model, written simply with short examples:\n",
      "\n",
      "1. Models predict text, not facts\n",
      "   - They guess the next words based on patterns in their training data. Their answers can be wrong or made up.\n",
      "   - Example: Ask “When did X happen?” and always double-check the date with a trusted source.\n",
      "\n",
      "2. Be clear and specific in your prompt\n",
      "   - Better input gives better output. Say what you want, the format, and any constraints.\n",
      "   - Example: Instead of “Write a story,” say “Write a 150‑word story for 8‑year‑olds about a brave kitten.”\n",
      "\n",
      "3. Verify important information\n",
      "   - Check facts, numbers, medical or legal advice with reliable sources or experts.\n",
      "   - Example: Use the model for a draft of health advice, then confirm details with a doctor or official guidance.\n",
      "\n",
      "4. Watch for bias and unfairness\n",
      "   - Models reflect biases in their training data. They can produce stereotyped or unfair content.\n",
      "   - Example: If a model gives a biased description of a group, correct it and consult diverse sources.\n",
      "\n",
      "5. Protect privacy and sensitive data\n",
      "   - Don’t share personal, confidential, or secret information in prompts.\n",
      "   - Example: Don’t paste someone’s full medical record or a private password.\n",
      "\n",
      "6. Use the model as an assistant, not an authority\n",
      "   - Treat outputs as suggestions or drafts to edit and improve.\n",
      "   - Example: Use generated text as a starting point for an essay, then revise tone and accuracy.\n",
      "\n",
      "7. Give step-by-step or constraints for complex tasks\n",
      "   - Break big tasks into smaller steps and ask the model to follow them.\n",
      "   - Example: Ask “List steps to plan a school field trip, then make a budget” rather than one vague prompt.\n",
      "\n",
      "8. Control creativity vs. accuracy\n",
      "   - Adjust instructions: ask for “creative” if you want imagination, or “precise” if you want facts.\n",
      "   - Example: “Write a realistic summary” vs. “Invent a fantasy scene.”\n",
      "\n",
      "9. Ask for sources or reasoning when needed\n",
      "   - Request explanations or references to see how the model reached an answer.\n",
      "   - Example: “Explain how you calculated that number” or “Cite sources for those facts.”\n",
      "\n",
      "10. Test and refine prompts\n",
      "    - Try different phrasings and compare answers. Iterate to improve results.\n",
      "    - Example: If the model misunderstands, reword the prompt and ask again.\n",
      "\n",
      "11. Respect rules, laws, and ethics\n",
      "    - Don’t use the model to create harmful content, cheat on tests, or break copyright or privacy laws.\n",
      "    - Example: Don’t ask the model to produce malware or to write someone else’s homework verbatim.\n",
      "\n",
      "12. Note limitations on older information\n",
      "    - Models have a cutoff date for training data and may not know recent events.\n",
      "    - Example: For current news, check a recent news site or official updates.\n",
      "\n",
      "If you want, I can show short prompt examples that follow these points for a specific task (lesson plan, email, summary). Which task would you like?"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = ChatOpenAI(\n",
    "    reasoning_effort=\"low\",\n",
    "    model=MODEL_NAME\n",
    ")\n",
    "\n",
    "# Set the prompts\n",
    "system_prompt = \"You are an elementary school teacher. You answer clearly and concisely, using examples.\"\n",
    "user_prompt = \"What are the key points to keep in mind when using a language model?\"\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]\n",
    "\n",
    "# Call the language model and display the result (streaming)\n",
    "for chunk in model.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8f72a4",
   "metadata": {},
   "source": [
    "## 4. Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5e83c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an excellent translator who translates from English to Spanish.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"You are an excellent translator who translates from {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "# Example: English -> Spanish\n",
    "messages = chat_prompt.format_messages(\n",
    "    input_language=\"English\",\n",
    "    output_language=\"Spanish\",\n",
    "    text=\"I love programming.\"\n",
    ")\n",
    "\n",
    "# The generated prompt messages\n",
    "messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed44087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me encanta programar.\n"
     ]
    }
   ],
   "source": [
    "# Use the prompt we created\n",
    "# Create the model\n",
    "model = ChatOpenAI(model=MODEL_NAME)\n",
    "\n",
    "# Call the language model\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# Display the result\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5294fb6",
   "metadata": {},
   "source": [
    "## 5. Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10fb0d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['apple', 'art', 'animal', 'answer', 'ability', 'amazing', 'accept', 'active', 'achieve', 'adapt']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# Create the model\n",
    "model = ChatOpenAI(model=MODEL_NAME)\n",
    "\n",
    "# Set the question\n",
    "user_prompt = \"Please output 10 English words that start with 'a', separated by commas.\"\n",
    "messages = [HumanMessage(content=user_prompt)]\n",
    "\n",
    "# Call the language model\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# Create an Output Parser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Parse with the Output Parser\n",
    "word_list = output_parser.parse(response.content)\n",
    "print(type(word_list))\n",
    "print(word_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01920589",
   "metadata": {},
   "source": [
    "## 6. Structured Output from Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "891cd218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'list'>\n",
      "List: ['apple', 'angle', 'answer', 'animal', 'arrival', 'abstract', 'abundant', 'attempt', 'antelope', 'allocate']\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# 1. Define the desired output data structure (schema) with Pydantic\n",
    "class WordList(BaseModel):\n",
    "    words: List[str] = Field(description=\"a list of words starting with 'a'\")\n",
    "\n",
    "# 2. Create the model\n",
    "model = ChatOpenAI(model=MODEL_NAME)  # A Structured Output–capable model is recommended\n",
    "\n",
    "# 3. Force structured output from the model (OpenAI parameters are set here)\n",
    "structured_llm = model.with_structured_output(WordList)\n",
    "\n",
    "# 4. Run\n",
    "user_prompt = \"Please output 10 English words that start with 'a'.\"\n",
    "response = structured_llm.invoke(user_prompt)\n",
    "\n",
    "# Check the result\n",
    "print(f\"Type: {type(response.words)}\")\n",
    "print(f\"List: {response.words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c94d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'dict'>\n",
      "Dict: {'words': ['apple', 'ant', 'anchor', 'artist', 'answer']}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1. Define the desired output structure as a JSON Schema (dictionary)\n",
    "json_schema = {\n",
    "    \"title\": \"WordList\",\n",
    "    \"description\": \"a list of words starting with 'a'\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"words\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"description\": \"List of words\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"words\"]\n",
    "}\n",
    "\n",
    "# 2. Create the model\n",
    "model = ChatOpenAI(model=MODEL_NAME)\n",
    "\n",
    "# 3. Create a structured-output model with method=\"json_mode\"\n",
    "# This makes the return value a plain dict instead of a Pydantic object\n",
    "structured_llm = model.with_structured_output(json_schema, method=\"json_mode\")\n",
    "\n",
    "# 4. Run\n",
    "# IMPORTANT: When using json_mode, clearly instruct the model to return JSON in the prompt\n",
    "user_prompt = \"Please output 5 English words that start with 'a' in JSON format.\"\n",
    "response = structured_llm.invoke(user_prompt)\n",
    "\n",
    "# Check the result\n",
    "print(f\"Type: {type(response)}\")\n",
    "print(f\"Dict: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a4375a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
